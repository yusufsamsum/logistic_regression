{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../datasets/question4/q4-train-dataset.csv\")\n",
    "test_df = pd.read_csv(\"../datasets/question4/q4-test-dataset.csv\")\n",
    "train = train_df.to_numpy()\n",
    "test = test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data  -= np.min(data,axis = 0)\n",
    "    data /= (np.max(data, axis = 0) - np.min(data,axis = 0))\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.to_numpy()\n",
    "test = test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[:, :-1]\n",
    "y_train = train[:, 29:30]\n",
    "x_test = test[:, :-1]\n",
    "y_test = test[:, 29:30]\n",
    "x_train = normalize(x_train)\n",
    "x_test = normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_func(x_train, weights):\n",
    "    return 1 / (1 + np.exp(- np.dot(x_train, weights))) ## p(y = 0| x, w)\n",
    "\n",
    "def update_rule(learning_rate, weights, coefficients):\n",
    "     ## new parameters  = old parameters + step size\n",
    "    weights = weights + learning_rate * coefficients\n",
    "    return weights\n",
    "\n",
    "def gradient_ascent(learning_rate, iteration_count, x_train, y_train, batch_size):\n",
    "    weights = np.random.normal(0, 0.01, x_train.shape[1]) ## N(0,0.01) distribution for initial weights\n",
    "    for i in range(iteration_count):\n",
    "        coefficients = 0  \n",
    "        for j in range(y_train.shape[0]):\n",
    "            likelihoods = y_train[j][0] - sigmoid_func(x_train[j], weights) ## predicted - likelihood\n",
    "            coefficients += np.dot(x_train[j].T, likelihoods) # coefficients for new weights\n",
    "            if( (j == y_train.shape[0] - 1) or ( j != 0 and j % batch_size == 0 )):\n",
    "                weights = update_rule(learning_rate, weights, coefficients)\n",
    "                gradient = 0\n",
    "    return weights\n",
    "\n",
    "def classifier(predictions):\n",
    "    for i in range(predictions.shape[0]):\n",
    "        if predictions[i] >= 0:\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "    return predictions\n",
    "\n",
    "def test(x_test, y_test, weights):\n",
    "    linear_predictions = np.dot(x_train, weights)\n",
    "    predictions = classifier(linear_predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def calc_accuracy(actual_values, predictions):\n",
    "    correct = 0\n",
    "    for pred, act in zip(predictions, actual_values):\n",
    "        if pred == act[0]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(actual_values))*100)\n",
    "\n",
    "def get_performance_metrics(actual_values, predictions):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    fdr = 0\n",
    "    fpr = 0\n",
    "    npv = 0\n",
    "    for pred, act in zip(predictions, actual_values):\n",
    "        if pred == 1:\n",
    "            if act[0] == 1.0:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if act[0] == 1.0:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    if tp == 0 and fp == 0:\n",
    "        precision =  0\n",
    "        recall = 0\n",
    "        fdr = 0\n",
    "        fpr = 0\n",
    "    else:\n",
    "        precision = ( (tp/(tp + fp)) )\n",
    "        recall = ( (tp / (tp + fn) ) )\n",
    "        fdr = ( (fp / (tp + fp)))\n",
    "        fpr = ( (fp / (fp + tn)))\n",
    "    if tn == 0 and fn == 0:\n",
    "        npv = 0\n",
    "    else:\n",
    "        npv = ( (tn / (tn + fn)) )\n",
    "    return (fp, tp, fn, tn, precision, recall, fdr, fpr, npv)\n",
    "\n",
    "def get_f_scores( beta, precision, recall ):\n",
    "    return ( (1 + np.power(beta, 2)) * precision * recall) / ((np.power(beta,2) * precision) + recall)\n",
    "         \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance_metric(fp, tp, fn, tn, precision, recall, fdr, fpr, npv, f1, f2):\n",
    "    print(\"True Positives: \", tp)\n",
    "    print(\"False Positives: \", fp)     \n",
    "    print(\"False Negatives: \", fn)  \n",
    "    print(\"True Negatives: \", tn) \n",
    "    print(\"Presicion: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"False Positive Rate: \", fpr)\n",
    "    print(\"False Discovery Rate: \", fdr)\n",
    "    print(\"Negative Predictive Value: \", npv)\n",
    "    print( \"F1: \", f1 )\n",
    "    print( \"F2: \", f2 )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_weights(weights):\n",
    "    sorted_weights = []\n",
    "    for i in range(len(weights)):\n",
    "        sorted_weights.append((i, abs(weights[i])))\n",
    "    sorted_weights.sort(key = operator.itemgetter(1))\n",
    "    return sorted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.arange(100, 1100, 100)\n",
    "learning_rates = [0.001, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for it in iterations:\n",
    "    for lr in learning_rates:\n",
    "        weight = gradient_ascent(lr, it, x_train, y_train, x_train.shape[0]) ## full batch\n",
    "        weights.append((it, lr, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for w in weights:\n",
    "    predictions.append((w[0], w[1], test(x_test, y_test, w[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [] \n",
    "for prediction in predictions:\n",
    "    acc = calc_accuracy(y_test, prediction[2])\n",
    "    accuracies.append( (prediction[0], prediction[1], acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100  learning rate: 0.001  accuracy: 96.03174603174604\n",
      "iteration 100  learning rate: 0.01  accuracy: 75.39682539682539\n",
      "iteration 100  learning rate: 0.1  accuracy: 95.23809523809523\n",
      "iteration 200  learning rate: 0.001  accuracy: 96.03174603174604\n",
      "iteration 200  learning rate: 0.01  accuracy: 96.82539682539682\n",
      "iteration 200  learning rate: 0.1  accuracy: 95.23809523809523\n",
      "iteration 300  learning rate: 0.001  accuracy: 96.82539682539682\n",
      "iteration 300  learning rate: 0.01  accuracy: 96.03174603174604\n",
      "iteration 300  learning rate: 0.1  accuracy: 86.5079365079365\n",
      "iteration 400  learning rate: 0.001  accuracy: 96.82539682539682\n",
      "iteration 400  learning rate: 0.01  accuracy: 96.03174603174604\n",
      "iteration 400  learning rate: 0.1  accuracy: 96.03174603174604\n",
      "iteration 500  learning rate: 0.001  accuracy: 96.82539682539682\n",
      "iteration 500  learning rate: 0.01  accuracy: 96.03174603174604\n",
      "iteration 500  learning rate: 0.1  accuracy: 96.03174603174604\n",
      "iteration 600  learning rate: 0.001  accuracy: 97.61904761904762\n",
      "iteration 600  learning rate: 0.01  accuracy: 96.03174603174604\n",
      "iteration 600  learning rate: 0.1  accuracy: 96.03174603174604\n",
      "iteration 700  learning rate: 0.001  accuracy: 97.61904761904762\n",
      "iteration 700  learning rate: 0.01  accuracy: 96.82539682539682\n",
      "iteration 700  learning rate: 0.1  accuracy: 89.68253968253968\n",
      "iteration 800  learning rate: 0.001  accuracy: 97.61904761904762\n",
      "iteration 800  learning rate: 0.01  accuracy: 96.82539682539682\n",
      "iteration 800  learning rate: 0.1  accuracy: 94.44444444444444\n",
      "iteration 900  learning rate: 0.001  accuracy: 96.82539682539682\n",
      "iteration 900  learning rate: 0.01  accuracy: 96.82539682539682\n",
      "iteration 900  learning rate: 0.1  accuracy: 96.82539682539682\n",
      "iteration 1000  learning rate: 0.001  accuracy: 96.82539682539682\n",
      "iteration 1000  learning rate: 0.01  accuracy: 96.82539682539682\n",
      "iteration 1000  learning rate: 0.1  accuracy: 96.03174603174604\n"
     ]
    }
   ],
   "source": [
    "for acc in accuracies:\n",
    "    print( \"iteration\" , acc[0], \" learning rate:\", acc[1], \" accuracy:\" , acc[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fp, tp, fn, tn, precision, recall, fdr, fpr, npv) = get_performance_metrics(y_test, predictions[27][2]) ## iteration = 1000, lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = get_f_scores(1, precision, recall)\n",
    "f2 = get_f_scores(2, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives:  60\n",
      "False Positives:  1\n",
      "False Negatives:  3\n",
      "True Negatives:  62\n",
      "Presicion:  0.9836065573770492\n",
      "Recall:  0.9523809523809523\n",
      "False Positive Rate:  0.015873015873015872\n",
      "False Discovery Rate:  0.01639344262295082\n",
      "Negative Predictive Value:  0.9538461538461539\n",
      "F1:  0.9677419354838709\n",
      "F2:  0.9584664536741213\n"
     ]
    }
   ],
   "source": [
    "print_performance_metric(fp, tp, fn, tn, precision, recall, fdr, fpr, npv, f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_weights = []\n",
    "for i in range(len(weights)):\n",
    "    if weights[i][1] == 0.001 and weights[i][0] == 1000:\n",
    "        for j in range(len(weights[i][2])):\n",
    "            full_weights.append(  weights[i][2][j] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.        ,  1.68047411],\n",
       "       [28.        ,  1.68506205],\n",
       "       [15.        ,  1.95950926],\n",
       "       [26.        ,  2.03871339],\n",
       "       [ 1.        ,  2.45340442],\n",
       "       [12.        ,  2.79422803],\n",
       "       [11.        ,  3.17434624],\n",
       "       [10.        ,  4.27014621],\n",
       "       [13.        ,  6.2043206 ],\n",
       "       [ 3.        ,  7.46958511]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_weights = get_sorted_weights(full_weights)\n",
    "np.array(sorted_weights)[19:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mini batch\n",
    "### from now an I choosed my learning rate 0.001 which is the best learning rate\n",
    "mini_batch_weights = []\n",
    "for it in iterations:\n",
    "    weight = gradient_ascent(0.001, it, x_train, y_train, 32)\n",
    "    mini_batch_weights.append((it, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_predictions = []\n",
    "for w in mini_batch_weights:\n",
    "    mini_batch_predictions.append((w[0], test(x_test, y_test, w[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_accuracies = [] \n",
    "for prediction in mini_batch_predictions:\n",
    "    acc = calc_accuracy(y_test, prediction[1])\n",
    "    mini_batch_accuracies.append( (prediction[0], acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100  accuracy 96.82539682539682\n",
      "iteration 200  accuracy 96.82539682539682\n",
      "iteration 300  accuracy 96.82539682539682\n",
      "iteration 400  accuracy 96.82539682539682\n",
      "iteration 500  accuracy 96.82539682539682\n",
      "iteration 600  accuracy 96.82539682539682\n",
      "iteration 700  accuracy 88.09523809523809\n",
      "iteration 800  accuracy 93.65079365079364\n",
      "iteration 900  accuracy 96.82539682539682\n",
      "iteration 1000  accuracy 96.03174603174604\n"
     ]
    }
   ],
   "source": [
    "for acc in mini_batch_accuracies:\n",
    "    print( \"iteration\" , acc[0], \" accuracy\", acc[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### iteration_count = 1000, learning_rate = 0.001\n",
    "(mini_batch_fp, mini_batch_tp, mini_batch_fn, mini_batch_tn, mini_batch_precision, mini_batch_recall, mini_batch_fdr, mini_batch_fpr, mini_batch_npv) = get_performance_metrics(y_test, mini_batch_predictions[9][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = get_f_scores(1, mini_batch_precision, mini_batch_recall)\n",
    "f2 = get_f_scores(2, mini_batch_precision, mini_batch_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives:  63\n",
      "False Positives:  5\n",
      "False Negatives:  0\n",
      "True Negatives:  58\n",
      "Presicion:  0.9264705882352942\n",
      "Recall:  1.0\n",
      "False Positive Rate:  0.07936507936507936\n",
      "False Discovery Rate:  0.07352941176470588\n",
      "Negative Predictive Value:  1.0\n",
      "F1:  0.9618320610687023\n",
      "F2:  0.9843750000000001\n"
     ]
    }
   ],
   "source": [
    "print_performance_metric(mini_batch_fp, mini_batch_tp, mini_batch_fn, mini_batch_tn, mini_batch_precision, mini_batch_recall, mini_batch_fdr, mini_batch_fpr, mini_batch_npv, f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stochastic gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_weights = []\n",
    "for it in iterations:\n",
    "    weight = gradient_ascent(0.001, it, x_train, y_train, 1) ## stochastic gradient ascent can be provided by setting the batch_size = 1\n",
    "    stochastic_weights.append((it, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_predictions = []\n",
    "for w in stochastic_weights:\n",
    "    stochastic_predictions.append((w[0], test(x_test, y_test, w[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_accuracies = [] \n",
    "for prediction in stochastic_predictions:\n",
    "    acc = calc_accuracy(y_test, prediction[1])\n",
    "    stochastic_accuracies.append( (prediction[0], acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100  accuracy 96.03174603174604\n",
      "iteration 200  accuracy 99.20634920634922\n",
      "iteration 300  accuracy 98.4126984126984\n",
      "iteration 400  accuracy 98.4126984126984\n",
      "iteration 500  accuracy 98.4126984126984\n",
      "iteration 600  accuracy 98.4126984126984\n",
      "iteration 700  accuracy 97.61904761904762\n",
      "iteration 800  accuracy 97.61904761904762\n",
      "iteration 900  accuracy 97.61904761904762\n",
      "iteration 1000  accuracy 97.61904761904762\n"
     ]
    }
   ],
   "source": [
    "for acc in stochastic_accuracies:\n",
    "    print( \"iteration\" , acc[0], \" accuracy\", acc[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fp, tp, fn, tn, precision, recall, fdr, fpr, npv) = get_performance_metrics(y_test, stochastic_predictions[9][1]) ## iteration = 1000, lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = get_f_scores(1, precision, recall)\n",
    "f2 = get_f_scores(2, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives:  61\n",
      "False Positives:  1\n",
      "False Negatives:  2\n",
      "True Negatives:  62\n",
      "Presicion:  0.9838709677419355\n",
      "Recall:  0.9682539682539683\n",
      "False Positive Rate:  0.015873015873015872\n",
      "False Discovery Rate:  0.016129032258064516\n",
      "Negative Predictive Value:  0.96875\n",
      "F1:  0.976\n",
      "F2:  0.9713375796178345\n"
     ]
    }
   ],
   "source": [
    "print_performance_metric(fp, tp, fn, tn, precision, recall, fdr, fpr, npv, f1, f2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
